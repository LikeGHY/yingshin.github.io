---
layout: post
title: "《SRE:Google运维解密》读书笔记"
date: 2019-12-14 18:41:48
tags: [spark]
---


## 6. 分布式系统的监控

### What

监控(monitoring)用于收集、处理、汇总系统的实时量化数据，例如请求的数量和用时、错误的数量和用时，以及处理用时，应用服务器的存活时间。

### Why

设计系统时，这个系统应当是可观测的。功能上，比如我们对比实验组和对照组的数据情况；性能上，哪些节点快，哪些节点慢，哪个流程处理快，哪个流程处理慢？

或者系统有故障的话，看哪些指标，怎么报警出来？怎么区分报警的级别（mail/短信/电话）？

一个良好的系统，这些观测点应当有一个控制台(dashboard).

监控和报警可以在系统有问题时通知我们，但是对于“有些东西看起来有点问题”的处理方式要谨慎，否则员工进入“狼来了”效应。而且如果在家里，紧急报警的处理影响个人生活。

能做到这一点真的很难，需要技术的驱动并且长期稳定的架构(此处指组织架构)，才能长久的沉淀出来一个好的监控报警系统，试想如果业务逻辑、架构总在变化，这些技术都沉淀不下来，更不用说辅助的监控系统了。至少目前我还没有见过做到这种效果的公司。

监控系统应当做的简单，不要试图做一些“魔法”系统，例如试图自动学习阈值或者检测原因。现在 AI 提的很多，有些监控系统也介绍怎么引入了 AI，我觉得纯粹是吃饱了没事干。

### 现象 vs 原因

我在做稳定性 case 分析时，经常发现这种分析路径：现象是线上流数据积压，原因则是依赖的第三方存储不稳定，第三方存储追查定位是更深层次的依赖问题，再往细里分析又归结为运维问题。同时原因也可能是多样的，在复杂环境里判断出来 root cause 是一个考验功力的事情，同时也要有决断，因为原因往往不在一个层面上，或者不是一个业务线，扯皮比处理时间还长。

另外就是要严格谨慎，明确原因后的 todo，是短期还是长期？是真的可以解决问题，还是只是为了另外一个项目的辅助收益？这条 case 分析->定位->判断->todo，往大了说，是一个很复杂的过程，需要深入去做。

![sym_and_reason](/assets/images/sym_and_reason.jpeg)

只有区分好这些，才算监控系统想清楚的第一步。

### 4 个黄金指标

延迟、流量、错误和饱和度(saturation)

这 4 个指标看到很有道理，能覆盖所有场景，不过具体重点看哪几个，具体分析吧。

### 其他：长尾、精度、简化

监控平均值往往是不够的，此时考虑分位值和直方图来看到长尾信息。

精度需要依赖 sla 确定，这是前提，因为监控难免带来资源消耗和代码入侵，这需要是共识。根据 sla 选择合适的精度。

及时跟进做系统简化，1个Q不用的监控项要及时删除。业务压力大，系统可以糙快猛的可劲上，不过上线后没人持续跟进，就不是一个好的现象了。这个准则适用于整个系统，我们需要能上线解决需求的pm和rd，也需要能分析盘点简化系统的pm和rd。

总结下，监控系统应当简单、易于理解，紧密结合系统不断改进，让我们能够及时发现系统问题，也尽量不要影响个人生活。